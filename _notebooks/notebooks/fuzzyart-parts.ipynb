{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b63189e",
   "metadata": {},
   "source": [
    "# FuzzyART in Parts\n",
    "\n",
    "In the other `FuzzyART` notebook, we implemented a `FuzzyART` module as a class with all of the requisite methods for running it as a standalone module.\n",
    "Here, we will look a little further down the rabbit hole to understand the moving parts of `FuzzyART` in finer detail.\n",
    "In programming terms, this might look more like the \"functional programming\" paradigm/pattern in that we wish to atomically look at each moving part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341ef4e",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "First, we load all of our dependencies for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a4d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From scikit-learn, for casting the data to 2D for visualization.\n",
    "# This is not how the data actually looks in 4D, but the best that we can do is to cast it to 2D such that relative distances are mostly maintained.\n",
    "from sklearn.manifold import TSNE\n",
    "# The most common way of importing matplotlib for plotting in Python\n",
    "from matplotlib import pyplot as plt\n",
    "# For manipulating axis tick locations\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d0a03",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next, we load our dataset!\n",
    "Here, we will use the UCI Iris dataset again as a relatively simple example.\n",
    "This time, we will modularize the preprocessing code a little more!\n",
    "First, we have the function to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73b5241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas for loading and manipulating data as a DataFrame\n",
    "import pandas as pd\n",
    "# For loading the iris dataset as an example\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    # Load the iris dataset as a DataFrame\n",
    "    iris = load_iris(as_frame=True)\n",
    "    # Extract the DataFrame from the dictionary the loader provides\n",
    "    data = iris['frame']\n",
    "    # Return the data as a DataFrame\n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "data = load_data()\n",
    "# Print the first several rows to get an idea of what it looks like\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe90ba",
   "metadata": {},
   "source": [
    "This dataset has 4 features, and it does include supervised labels.\n",
    "\n",
    "Working directly with `DataFrame`s is good for data analysis, but for machine learning, we often have to think more carefully about how we handle our datasets in efficient ways during training and inference, especially when words like \"mini-batching\", \"memory pinning\", and \"transforms\" come up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76e263",
   "metadata": {},
   "source": [
    "### Data Container\n",
    "\n",
    "For a simple demonstration, we create a convenient container for the preprocessed samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22c02f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PreprocessedDataset in module __main__:\n",
      "\n",
      "class PreprocessedDataset(builtins.object)\n",
      " |  PreprocessedDataset(x: torch.Tensor, y: torch.Tensor) -> None\n",
      " |\n",
      " |  PreprocessedDataset(x: torch.Tensor, y: torch.Tensor)\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __init__(self, x: torch.Tensor, y: torch.Tensor) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  dim(self)\n",
      " |      # The \"original\" (not complement-coded) data feature dimension\n",
      " |\n",
      " |  dim_cc(self)\n",
      " |      # The complement-coded dimension\n",
      " |\n",
      " |  n_samples(self)\n",
      " |      # The number of samples\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'x': <class 'torch.Tensor'>, 'y': <class 'torch.Ten...\n",
      " |\n",
      " |  __dataclass_fields__ = {'x': Field(name='x',type=<class 'torch.Tensor'...\n",
      " |\n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __match_args__ = ('x', 'y')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The PyTorch library containing neural network utilities and the Tensor datatype\n",
    "import torch\n",
    "\n",
    "# Dataclass for a structured way of passing around a dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Create a simple container for our preprocessed data with some introspection methods\n",
    "@dataclass\n",
    "class PreprocessedDataset:\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "    # The number of samples\n",
    "    def n_samples(self):\n",
    "        return y.shape[0]\n",
    "\n",
    "    # The \"original\" (not complement-coded) data feature dimension\n",
    "    def dim(self):\n",
    "        return int(x.shape[1] / 2)\n",
    "\n",
    "    # The complement-coded dimension\n",
    "    def dim_cc(self):\n",
    "        return x.shape[1]\n",
    "\n",
    "# Check out added methods from the decorator.\n",
    "help(PreprocessedDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b886eb2",
   "metadata": {},
   "source": [
    "Let's create a quick dataset with some random dummy data to see what creating a \"formal\" dataset class gets us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bde148a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessedDataset(x=tensor([[0.0290, 0.4019],\n",
       "        [0.2598, 0.3666],\n",
       "        [0.0583, 0.7006],\n",
       "        [0.0518, 0.4681],\n",
       "        [0.6738, 0.3315]]), y=tensor([0, 1, 1, 1, 0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy dataset with some dimension and number of samples\n",
    "dim = 2\n",
    "n_samples = 5\n",
    "# Set a manual random seed for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "dummy_data = PreprocessedDataset(\n",
    "    torch.rand((n_samples, dim)),\n",
    "    torch.randint(0, 3, (n_samples,))\n",
    ")\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a80c4",
   "metadata": {},
   "source": [
    "One thing that we notice is that we got a fancier string representation (`__repr__`) of the object when it's displayed on the terminal, and we didn't have to write it ourselves.\n",
    "This is a simple example, but it is one of the beneficial things that we can get when thinking a little bit about how we handle datasets (in this case, hooking into a Python standard library `dataclass` decorator).\n",
    "\n",
    "Creating object to hold your data, such as annotating with the standard library decorator `dataclasses.@dataclass`, is a useful trick to inherit some boilerplate machinery.\n",
    "In PyTorch, the most universal API to utilize and interface with datasets is in [`torch.utils.data`](https://docs.pytorch.org/docs/stable/data.html), which includes the `Dataset` and `DataLoader` classes.\n",
    "For now, we'll stick with this simpler implementation for illustration.\n",
    "\n",
    "Next up is how to preprocess the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3147f8",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Now we define the function to preprocess the data.\n",
    "In this example, we do this on the full batch rather than incrementally for the following reason:\n",
    "\n",
    "`FuzzyART` uses complement coding, which maps $x \\rightarrow [x, 1-x]$ and is bounded in $[0, 1]$.\n",
    "To do this, we need the original $x$ to be also be bounded inside $[0, 1]$.\n",
    "Most real data is not neatly normalized and bounded, so we need to do it ourselves at some point.\n",
    "However, that requires knowing the bounds of the full data in advance!\n",
    "\n",
    "Even though we are working with an incremental algorithm, we have the luxury of having the all of the Iris dataset up-front, and the dataset surely isn't going to change any time soon.\n",
    "This is not always the case, especially if you are dealing with streaming datasets, where you are incrementally provided a sample one at a time!\n",
    "In those cases, you have two options:\n",
    "\n",
    "1. Know the statistics of the dataset in advance (e.g., the upper and lower bounds) and preprocess each sample incrementally off of that.\n",
    "2. Use some sort of intelligent normalization scheme that enforces the bounds of the data to $[0, 1]$, such as through incorporating a limiting function like the sigmoid function\n",
    "$\\sigma = \\dfrac{1}{1+e^{-x}}$\n",
    "or some other hard limiting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9feb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreprocessedDataset(x=tensor([[0.3611, 0.2083, 0.4915,  ..., 0.7917, 0.5085, 0.5833],\n",
       "        [0.0278, 0.5000, 0.0508,  ..., 0.5000, 0.9492, 0.9583],\n",
       "        [0.5556, 0.5417, 0.6271,  ..., 0.4583, 0.3729, 0.3750],\n",
       "        ...,\n",
       "        [0.5278, 0.3333, 0.6441,  ..., 0.6667, 0.3559, 0.2917],\n",
       "        [0.8056, 0.4167, 0.8136,  ..., 0.5833, 0.1864, 0.3750],\n",
       "        [0.1111, 0.5000, 0.1017,  ..., 0.5000, 0.8983, 0.9583]],\n",
       "       dtype=torch.float64), y=tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 2, 0, 2, 1, 1, 2, 2, 0, 2, 1, 1, 1, 1, 0, 1,\n",
       "        0, 2, 0, 1, 0, 2, 0, 0, 2, 2, 1, 2, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 1,\n",
       "        1, 1, 2, 0, 1, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 1, 2, 1, 0, 0, 1, 0, 2, 2,\n",
       "        0, 0, 2, 0, 2, 1, 0, 0, 1, 2, 0, 2, 1, 0, 1, 1, 1, 0, 0, 1, 2, 0, 1, 1,\n",
       "        2, 1, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 0, 1, 0, 0,\n",
       "        0, 2, 1, 0, 1, 2, 1, 1, 0, 2, 0, 0, 0, 1, 1, 0, 2, 2, 1, 2, 1, 2, 2, 0,\n",
       "        2, 0, 2, 2, 2, 0], dtype=torch.int32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy for handling numpy arrays\n",
    "import numpy as np\n",
    "# A sklearn utility for handling normalization of data automatically\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define a preprocess function that returns a PreprocessedDataset\n",
    "def preprocess(\n",
    "    data: pd.DataFrame,\n",
    "    shuffle: bool = True,\n",
    "    random_seed: int = 12345,\n",
    ") -> PreprocessedDataset:\n",
    "    # Shuffle the data if necessary\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Whether shuffled or not, separate the labels\n",
    "    labels = torch.tensor(data.pop('target'), dtype=torch.int)\n",
    "\n",
    "    # Initialize the scalar and update the values in-place to be normalized between [0, 1]\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Linearly normalize the data and put it into a tensor\n",
    "    data_cc = torch.tensor(scaler.fit_transform(data))\n",
    "\n",
    "    # Complement code the data by appending the vector [1-x] along the feature dimension\n",
    "    data_cc = torch.cat((data_cc, 1 - data_cc), dim=1)\n",
    "\n",
    "    # What we get is a list of 8-dimensional samples\n",
    "    return PreprocessedDataset(data_cc, labels)\n",
    "\n",
    "# Preprocess the data and check it out\n",
    "data_cc = preprocess(data)\n",
    "data_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378bee6b",
   "metadata": {},
   "source": [
    "Now that that we have our data defined and preprocessed (linearly normalized and complement coded), we can define some parts of our `FuzzyART` module!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e67ff7",
   "metadata": {},
   "source": [
    "## FuzzyART\n",
    "\n",
    "We need to define the following things to have a complete FuzzyART module:\n",
    "\n",
    "1. **Weights**: How the weights are defined in memory.\n",
    "This is a growing data structure in the sense that FuzzyART instantiates new nodes when necessary, so whatever it is that we do, we need a mechanism to append weights.\n",
    "2. **Activation**: The FuzzyART activation function; this takes the weights and input sample, and it spits out a number.\n",
    "3. **Match**: The FuzzyART match function.\n",
    "This also is a function of the weights and current sample, and it returns a number.\n",
    "4. **Learning**: One a winning node is selected, we need to define what learning is; the weight update function returns a new weight as a function of the old weight, the current sample, and the learning rate.\n",
    "5. **Match Rule/Competition**: This function iterates over all of the weights, calculating their activation and match values, selecting a winning node, and updating it.\n",
    "This is essentially the main loop that we will call during training and inference, performing the FuzzyART \"match rule.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1dc824",
   "metadata": {},
   "source": [
    "What this translates to is an empty API to \"fill out\" in steps.\n",
    "We often use the abstract class pattern in software development to do such a thing, which is implemented in Python using the standard library module `abc`, such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5194e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ABC, abstractmethod\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mFuzzyARTBase\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mABC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# A method to initialize a new category, however it the weights are defined\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;129;43m@abstractmethod\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43mgrow\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mFuzzyARTBase\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# The training method\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:\u001b[43mTensor\u001b[49m):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# After training, the classification method\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class FuzzyARTBase(ABC):\n",
    "\n",
    "    # A method to initialize a new category, however it the weights are defined\n",
    "    @abstractmethod\n",
    "    def grow(self, x: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    # The activation function for a single sample and weight\n",
    "    @abstractmethod\n",
    "    def activation(self, x: torch.Tensor, j: int) -> torch.float:\n",
    "        pass\n",
    "\n",
    "    # The match function for a single sample and weight\n",
    "    @abstractmethod\n",
    "    def match(self, x: torch.Tensor, j: int) -> torch.float:\n",
    "        pass\n",
    "\n",
    "    # The learning function once a weight update has\n",
    "    @abstractmethod\n",
    "    def learn(self, x: torch.Tensor, j: int) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    # The FuzzyART match rule (competition) loop, which\n",
    "    @abstractmethod\n",
    "    def competition(self, x: torch.Tensor, j: int) -> int:\n",
    "        pass\n",
    "\n",
    "    # The training method\n",
    "    @abstractmethod\n",
    "    def train(self, x:torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    # After training, the classification method\n",
    "    @abstractmethod\n",
    "    def classify(self, x:torch.Tensor) -> int:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffe49a",
   "metadata": {},
   "source": [
    "For a full implementation of almost exactly this, check out the [notebook at this link](https://art-book-online.github.io/notebooks/fuzzyart-pytorch/) that does this in a single `FuzzyART` class with PyTorch tensor weights.\n",
    "\n",
    "In this notebook we'll be inspecting each of these functions individually, so we'll modularize it into a set of functions that take all of the info that they need and spit out the relevant information.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "This isn't the most efficient method of implementing `FuzzyART`.\n",
    "In fact, it might actually be the slowest way of going about it!\n",
    "However, that is not the point of this exercise; the point is to get an understanding of each of the working parts so that you can go forth and implement it however you need in your own application!\n",
    "</div>\n",
    "\n",
    "As a complete side note, you could *conceivably* combine both methods (creating an abstract base class and implementing the methods *after the fact*) with something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e332ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if `obj` has 'added_method': False\n",
      "Checking if `obj` has an 'added_method': True\n",
      "Output of 'added_method': My x is: 1\n"
     ]
    }
   ],
   "source": [
    "# The types module has all sorts of black magic within\n",
    "from types import MethodType\n",
    "\n",
    "# Define some basic class with data\n",
    "class MyClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "# Initialize the object\n",
    "obj = MyClass(1)\n",
    "\n",
    "# Verify that the class doesn't have the method we need\n",
    "print(f\"Checking if `obj` has 'added_method': {hasattr(MyClass, 'added_method')}\")\n",
    "\n",
    "# Define a dangling method after defining the class with a \"self\" argument\n",
    "def added_method(self):\n",
    "    return f\"My x is: {self.x}\"\n",
    "\n",
    "# Attach the function as a method of the instantiated object\n",
    "obj.check = MethodType(added_method, obj)\n",
    "\n",
    "# Verify that the instantiated object has the method, and see what it outputs\n",
    "print(f\"Checking if `obj` has an 'added_method': {hasattr(obj, 'check')}\")\n",
    "print(f\"Output of 'added_method': {obj.check()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9912254",
   "metadata": {},
   "source": [
    "That's not what we'll do here, but it's neat what you can do with code when you think outside the box!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b06f10",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "Our weights are defined as a grow matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c36b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bea3c26",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "This notebook is a work in progress!\n",
    "If you see this, it means that there is more to come for this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-book-online",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
