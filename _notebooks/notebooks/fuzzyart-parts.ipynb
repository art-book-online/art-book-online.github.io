{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b63189e",
   "metadata": {},
   "source": [
    "# FuzzyART in Parts\n",
    "\n",
    "In the other `FuzzyART` notebook, we implemented a `FuzzyART` module as a class with all of the requisite methods for running it as a standalone module.\n",
    "Here, we will look a little further down the rabbit hole to understand the moving parts of `FuzzyART` in finer detail.\n",
    "In programming terms, this might look more like the \"functional programming\" paradigm/pattern in that we wish to atomically look at each moving part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341ef4e",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "First, we load all of our dependencies for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3a4d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For manipulating local paths in an object-oriented way\n",
    "from pathlib import Path\n",
    "# Dataclass for a structured way of passing around a dataset\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# The PyTorch library containing neural network utilities and the Tensor datatype\n",
    "import torch\n",
    "# A convenient import of Tensor so that we don't have to write torch.Tensor every time\n",
    "from torch import Tensor\n",
    "# Pandas for loading and manipulating data as a DataFrame\n",
    "import pandas as pd\n",
    "# Numpy for handling numpy arrays (i.e., matplotlib doesn't understand Tensor types, but it does know numpy.nparray)\n",
    "import numpy as np\n",
    "\n",
    "# A sklearn utility for handling normalization of data automatically\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# From scikit-learn, for casting the data to 2D for visualization.\n",
    "# This is not how the data actually looks in 4D, but the best that we can do is to cast it to 2D such that relative distances are mostly maintained.\n",
    "from sklearn.manifold import TSNE\n",
    "# For loading the iris dataset as an example\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# The most common way of importing matplotlib for plotting in Python\n",
    "from matplotlib import pyplot as plt\n",
    "# For manipulating axis tick locations\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d0a03",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Next, we load our dataset!\n",
    "Here, we will use the UCI Iris dataset again as a relatively simple example.\n",
    "This time, we will modularize the preprocessing code a little more!\n",
    "First, we have the function to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a73b5241",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IrisData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m() -> \u001b[43mIrisData\u001b[49m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Load the iris dataset as a DataFrame\u001b[39;00m\n\u001b[32m      3\u001b[39m     iris = load_iris(as_frame=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# Extract the dataframe from the dictionary the loader provides\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'IrisData' is not defined"
     ]
    }
   ],
   "source": [
    "def load_data() -> IrisData:\n",
    "    # Load the iris dataset as a DataFrame\n",
    "    iris = load_iris(as_frame=True)\n",
    "    # Extract the dataframe from the dictionary the loader provides\n",
    "    data = iris['frame']\n",
    "    # Extract the target as a vector of integer labels\n",
    "    # labels = iris.target\n",
    "    # Return the data container\n",
    "    # return IrisData(data, labels)\n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "data = load_data()\n",
    "# Print the first several rows to get an idea of what it looks like\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe90ba",
   "metadata": {},
   "source": [
    "Then we have the function to preprocess the data.\n",
    "In this example, we do this on the full batch rather than incrementally for the following reason:\n",
    "\n",
    "`FuzzyART` uses complement coding, which maps $x \\rightarrow [x, 1-x]$ and is bounded in $[0, 1]$.\n",
    "To do this, we need the original $x$ to be also be bounded inside $[0, 1]$.\n",
    "Most real data is not neatly normalized and bounded, so we need to do it ourselves at some point.\n",
    "However, that requires knowing the bounds of the full data in advance!\n",
    "\n",
    "Even though we are working with an incremental algorithm, we have the luxury of having the all of the Iris dataset up-front, and the dataset surely isn't going to change any time soon.\n",
    "This is not always the case, especially if you are dealing with streaming datasets, where you are incrementally provided a sample one at a time!\n",
    "In those cases, you have two options:\n",
    "\n",
    "1. Know the statistics of the dataset in advance (e.g., the upper and lower bounds) and preprocess each sample incrementally off of that.\n",
    "2. Use some sort of intelligent normalization scheme that enforces the bounds of the data to $[0, 1]$, such as through incorporating a limiting function like the sigmoid function\n",
    "$\\sigma = \\dfrac{1}{1+e^{-x}}$\n",
    "or some other hard limiting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9feb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IrisData(x=tensor([[0.3611, 0.2083, 0.4915,  ..., 0.7917, 0.5085, 0.5833],\n",
       "        [0.0278, 0.5000, 0.0508,  ..., 0.5000, 0.9492, 0.9583],\n",
       "        [0.5556, 0.5417, 0.6271,  ..., 0.4583, 0.3729, 0.3750],\n",
       "        ...,\n",
       "        [0.5278, 0.3333, 0.6441,  ..., 0.6667, 0.3559, 0.2917],\n",
       "        [0.8056, 0.4167, 0.8136,  ..., 0.5833, 0.1864, 0.3750],\n",
       "        [0.1111, 0.5000, 0.1017,  ..., 0.5000, 0.8983, 0.9583]]), y=0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    0\n",
       "146    2\n",
       "147    2\n",
       "148    2\n",
       "149    0\n",
       "Name: target, Length: 150, dtype: int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class IrisData:\n",
    "    x: Tensor\n",
    "    y: np.array\n",
    "\n",
    "def preprocess(\n",
    "    data: pd.DataFrame,\n",
    "    shuffle: bool = True,\n",
    "    random_seed: int = 12345,\n",
    "):\n",
    "    # Shuffle the data if necessary\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        # data['Labels'] = iris.target\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "    # Whether shuffled or not, sepearate the labels\n",
    "    labels = data.pop('target')\n",
    "\n",
    "    # Intialize the scalar and update the values in-place to be normalized between [0, 1]\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(data))\n",
    "\n",
    "    # Complement code the data by pushing it into a Tensor\n",
    "    data_cc = torch.Tensor(data.values)\n",
    "    # and appending the vector [1-x] along the feature dimension\n",
    "    data_cc = torch.cat((data_cc, 1 - data_cc), dim=1)\n",
    "    # What we get is a list of 8-dimensional samples\n",
    "    return IrisData(data_cc, labels)\n",
    "\n",
    "data_cc = preprocess(data)\n",
    "data_cc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea3c26",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "This notebook is a work in progress!\n",
    "If you see this, it means that there is more to come for this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art-book-online",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
