---
title: Why does this website exist?
subtitle: An explanation for the purpose of this website and the ART book.
layout: single
# classes: wide
toc: true
toc_icon: question
toc_sticky: true

# Date
date: 2025-12-30
# Date updated
lastmod: 2025-12-30

# authors: ["admin"]
categories: ["posts", "about"]
tags: ["posts"]
featured: true
# profile: true
# commentable: true
# share: true

excerpt: "An explanation for the purpose of this website and the ART book."

header:
    teaser: "/assets/icons/book-512.png"
    caption: "The ART Book"
---

This website, and the ART Book for that matter, could use a little bit of context.

## Why?

1. The ART Book exists because they authors wanted a book that taught, explained, and served as a reference of adaptive resonance theory algorithms for engineers and machine learning practitioners, and there wasn't one.
So they wrote one!
1. This website exists as a homepage for the book and a set of interactive exercises for the reader to get hands-on with the material, because that is the best way to learn machine learning material in general.

However, you might then say, "okay, a book and website are all well and good, but *what on Earth are you talking about? What is ART? What are ART algorithms?*"

This story begins with a relatively small laboratory in a relatively small university doing machine learning research in a relatively small town in Missouri, which is relatively average-sized.

## Origins

At the time of this writing, Dr. Sasha Petrenko (the author - hi, that's me!) works as an assistant research professor at Missouri S&T, an engineering university in the relatively small town of Rolla, Missouri, whose closest metropolitan city is St. Louis.

{: .notice--info}
Fun Fact: Missouri is a state in the U.S.! Neat, right?

The Applied Computational Intelligence Laboratory (ACIL) that he works in at Missouri S&T is a lab of applied machine learning research, specializing in the research and applications of a class of algorithms based upon Adaptive Resonance Theory (ART).
ART algorithms are a bit of a niche field in machine learning and AI, and they're motivated a little differently than many other modern AI methods.

## AI

Once upon a time, people were tackling the problem of artificial intelligence (AI).
They still do, in fact, but they used to as well.

One of the first things one would do, naturally, when trying to study how intelligence works and how to replicate it in artificial intelligent systems is to study *natural intelligence*.
The evidence that it is indeed possible to have an intelligent system that does intelligent things with intelligent thoughts is right between our ears, as they say.
Well, that may not be as true for some people, but the point still stands.

In the study of the mammalian brain, we have identified that the brain has evolved solutions to some universal problems that face any intelligent system, as it turns out.
This study is the basis for a lot of the most successful AI algorithms today, and one common motif is that rather than "hard coding" a solution to any and every problem under the sun, a winning strategy is to create a system that can *learn on its own* and give it the machinery to update its parameters to suit the problem at hand.

{: .notice}
In biological terms, this is adaptation.
In AI terms, this is machine learning.

There are a lot of "camps" in the field of machine learning, most of which began with biologically-plausible inspirations (e.g., neural networks), and many of which have moved away from biology to statistical interpretations (e.g., deep learning).
Machine learning has become dominated in recent years by these statistical learning methods with increasingly large deep learning models, deservedly so because of the wonders of what it has accomplished; passiblely intelligent chatbots and beating humans at chess and go were science fiction in the not too distant past!

However, these techniques have draw backs that human brains don't have that make it worthwhile sometimes to "go back to the drawing board" and think outside the box on how to make them more brain-like.
That's what brings us to ART!

## What is ART?

We know that the brain is a *lot* of neurons that are *massively interconnected*.
That's all well and good, but the question then becomes: what in the heck are they doing?
Like, truly, what functions are they computing, and how does all of that mess turn into my conscious thought?
Furthermore, though these neurons are indeed very interconnected, *every neuron isn't directly connected to ever other one* in a giant interconnected mess; the brain has some *structure* that is important in how it works and how we get to, you know, *the consciousness thing*.

Rather than tackling the literal *hard problem of consciousness* right now, let's step back and ask some more manageable questions, such as: How do these neurons learn? How do they store and process information? And very importantly, *how do the neurons know what to do?*
It's not like each neuron, after it's fired a few times, has an instructor tapping it on the shoulder, saying "no, no, no, you should do it this way instead."
How would the instructor know what the right answer is?
Does the instructor have an instructor?
It would be [turtles all the way down](https://en.wikipedia.org/wiki/Turtles_all_the_way_down)!

The brain clearly has some *self-organizing* machinery at work; the monkey wasn't born already knowing the ins and outs of what a banana is and that it is yummy, it figured that out for itself along the way!
The question next becomes: *how*?

Well, ART is a set of theories and models whose objectives are to answer the questions of "what are the simplest models that achieve this self-organizing behavior?"
In other words, "how do you wire up a bunch of neurons so that this self-organizing learning becomes an *emergent phenomenon*?"

These questions and investigations into the *self-organizing neuroscience* side of things got a lot of attention by Boston University's [Dr. Stephen Grossberg](https://sites.bu.edu/steveg/), so his name appears *a lot* in the ART literature from the neuroscience and psychology side of things.
His wife [Dr. Gail Carpenter](http://techlab.bu.edu/members/gail/) spearheaded a lot of the applications of this work in machine learning research and applied algorithms, so her name appears *a lot* in the ART literature from the applied side of things.

Though these two names are geometric center of this study, a lot of research groups (such as teh ACIL, as it turns out!) have expanded upon this field over the years.

## The ART Book

I didn't invent this field, my lab didn't invent this field, and lots of other people have worked and written about ART theory and practice.
There are already books that talk about ART out there, such as Grossberg's own magnum opus [Conscious Brain, Resonant Brain](https://www.goodreads.com/book/show/57175350-conscious-mind-resonant-brain).

So why another book?

{: .notice--info}
Whoa, this is an info block!

{: .notice--warning}
Oops, this is an warning block.
No bueno!

{: .notice}
This is just a plain notice block.
Big whoop!
